{\rtf1\ansi\ansicpg1252\cocoartf1404\cocoasubrtf340
{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;}
\margl1440\margr1440\vieww10800\viewh8400\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\fs24 \cf0 Run softmax.py from folder containing training and test data in .csv format. Uses gradient descent to create softmax fit for data sets with more than two classes. Takes MxN+1 table where m is number of set items, and n is number of features, and label is final column. Automatically converts integer labels to one-hot for softmax analysis.\
\
Tested on MNIST, CIFAR-10 and spambase datasets. For spambase and MNIST, learning rate of 1e-5 was appropriate. For CIFAR-10, learning rate of 1e-11 was appropriate. \
\
softmax.py uses Python 2, including the python dependencies: pandas, NumPy, spicy\
\
Run as follows:\
\
$ python ./softmax.py <trainset.csv> <testset.csv> <learning rate> <regularize value> <iterations>}